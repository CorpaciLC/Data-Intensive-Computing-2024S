{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f332df40-a2c1-4774-bf1a-08b760c9a67d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import RegexTokenizer, CountVectorizer, StopWordsRemover, HashingTF, IDF, ChiSqSelector, CountVectorizer\n",
    "from pyspark.ml.feature import Tokenizer, Normalizer, StringIndexer\n",
    "from pyspark.sql.functions import col, lower\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd200b63-8ca2-4222-817e-37d5ea6fa35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_dataset = \"/user/dic24_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "full_dataset = \"/user/dic24_shared/amazon-reviews/full/reviewscombined.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "035eca20-70d1-4a8f-9a00-47ff9624a6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File read successfully.\n",
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- helpful: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      "\n",
      "+----------+--------------------+-------+-------+--------------------+-----------+--------------+--------------------+-------------------+--------------+\n",
      "|      asin|            category|helpful|overall|          reviewText| reviewTime|    reviewerID|        reviewerName|            summary|unixReviewTime|\n",
      "+----------+--------------------+-------+-------+--------------------+-----------+--------------+--------------------+-------------------+--------------+\n",
      "|0981850006|Patio_Lawn_and_Garde| [6, 7]|    5.0|This was a gift f...| 12 3, 2009|A2VNYWOPJ13AFP|Amazon Customer \"...|             Delish|    1259798400|\n",
      "|B00002N66D|Patio_Lawn_and_Garde| [1, 1]|    5.0|This is a very ni...| 12 3, 2012|A2E5XXXC07AGA7|               James|      Nice spreader|    1354492800|\n",
      "|B00002N67U|Patio_Lawn_and_Garde| [0, 1]|    1.0|The metal base wi...|08 13, 2008|A16PX63WZIEQ13|             Finaldx|Terrible spike base|    1218585600|\n",
      "|B00002N6AN|Patio_Lawn_and_Garde| [0, 0]|    4.0|For the most part...| 10 1, 2009|A2OSWM3522VARA|Wayne Allen \"Moto...|  gets the job done|    1254355200|\n",
      "|B00002N8K3|Patio_Lawn_and_Garde| [4, 5]|    1.0|This hose is supp...|07 13, 2013|A2SX9YPPGEUADI|HappyCamper \"Happ...|          The worst|    1373673600|\n",
      "+----------+--------------------+-------+-------+--------------------+-----------+--------------+--------------------+-------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Text_Classification\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "try:\n",
    "    df = spark.read.json(partial_dataset)\n",
    "    print(\"File read successfully.\")\n",
    "    \n",
    "    # Show the schema and some data\n",
    "    df.printSchema()\n",
    "    df.show(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8824c96-9b4e-4f26-9670-3a9ba0aaa784",
   "metadata": {},
   "source": [
    "# Part 2: Datasets/DataFrames: Spark ML and Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10c26a83-efb3-4811-9efb-5ede4f8209eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casefolding\n",
    "df = df.withColumn(\"reviewText\", lower(col(\"reviewText\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb22f5e2-c855-454c-acf1-49673b463608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сonverting category to numeric\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "\n",
    "# Creating the pipeline:\n",
    "\n",
    "# 1. tokenization\n",
    "tokenizer = RegexTokenizer(inputCol=\"reviewText\", outputCol=\"words\", pattern=r'\\s+|\\t+|\\d+|[(){}.!?,;:+=-_\"\\`~#@&*%€$§\\\\/]+', gaps=True)\n",
    "\n",
    "# 3. Stopwords removal\n",
    "stopwords_remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"filtered_words\")\n",
    "\n",
    "# 4. tf-idf calculation with CountVectorizer\n",
    "hashingTF = CountVectorizer(inputCol=stopwords_remover.getOutputCol(), outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=hashingTF.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "# # 4. TF-IDF calculation with HashingTF\n",
    "# hashingTF = HashingTF(inputCol=stopwords_remover.getOutputCol(), outputCol=\"rawFeatures\", numFeatures=20)\n",
    "# idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# 5. Chi-square\n",
    "selector = ChiSqSelector(numTopFeatures=2000, featuresCol=idf.getOutputCol(),\n",
    "                         outputCol=\"selectedFeatures\", labelCol=indexer.getOutputCol())\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[indexer, tokenizer, stopwords_remover, hashingTF, idf, selector])\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "model = pipeline.fit(df)\n",
    "fit_time = time.time() - start_time\n",
    "print(f'fit_time={fit_time}')\n",
    "\n",
    "# Transform the test data\n",
    "result = model.transform(df)\n",
    "transform_time = time.time() - fit_time - start_time\n",
    "print(f'transform_time={transform_time}')\n",
    "\n",
    "# Extract the CountVectorizer model from the pipeline\n",
    "count_vectorizer_model = model.stages[3]\n",
    "vocab = count_vectorizer_model.vocabulary\n",
    "\n",
    "# Get the selected feature indices\n",
    "selected_indices = result.select(\"selectedFeatures\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Convert the indices to terms\n",
    "selected_terms = [vocab[index] for indices in selected_indices for index in indices.indices]\n",
    "\n",
    "    \n",
    "with open(\"output_ds.txt\", \"w\") as f:\n",
    "    for term in selected_terms:\n",
    "        # print(term)\n",
    "        f.write(f\"{term} \")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
