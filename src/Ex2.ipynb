{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f332df40-a2c1-4774-bf1a-08b760c9a67d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, CountVectorizer, StopWordsRemover, HashingTF, IDF, ChiSqSelector, CountVectorizer\n",
    "from pyspark.ml.feature import Tokenizer, Normalizer, StringIndexer\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "from pyspark.sql.functions import col, lower\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd200b63-8ca2-4222-817e-37d5ea6fa35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_dataset = \"/user/dic24_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "full_dataset = \"/user/dic24_shared/amazon-reviews/full/reviewscombined.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "035eca20-70d1-4a8f-9a00-47ff9624a6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File read successfully.\n",
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|            category|          reviewText|\n",
      "+--------------------+--------------------+\n",
      "|Patio_Lawn_and_Garde|This was a gift f...|\n",
      "|Patio_Lawn_and_Garde|My husband rarely...|\n",
      "|Patio_Lawn_and_Garde|This guy knows hi...|\n",
      "|Patio_Lawn_and_Garde|THIS CAN BE FOUND...|\n",
      "|Patio_Lawn_and_Garde|Quite good partic...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Text_Classification\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "try:\n",
    "    df = spark.read.json(full_dataset).select(\"category\", \"reviewText\")\n",
    "    print(\"File read successfully.\")\n",
    "    df.printSchema()\n",
    "    df.show(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c336398-2c4d-4622-a71f-c8693594b730",
   "metadata": {},
   "source": [
    "# Part 1: RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8824c96-9b4e-4f26-9670-3a9ba0aaa784",
   "metadata": {},
   "source": [
    "# Part 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10c26a83-efb3-4811-9efb-5ede4f8209eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casefolding\n",
    "df = df.withColumn(\"reviewText\", lower(col(\"reviewText\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb22f5e2-c855-454c-acf1-49673b463608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сonverting category to numeric\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "\n",
    "# Creating the pipeline:\n",
    "\n",
    "# 1. tokenization\n",
    "tokenizer = RegexTokenizer(inputCol=\"reviewText\", outputCol=\"words\", pattern=r'\\s+|\\t+|\\d+|[(){}.!?,;:+=-_\"\\`~#@&*%€$§\\\\/\\'-]+', gaps=True)\n",
    "\n",
    "# 2. Stopwords removal\n",
    "stopwords_remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"filtered_words\")\n",
    "\n",
    "# 3. tf-idf calculation with CountVectorizer\n",
    "hashingTF = CountVectorizer(inputCol=stopwords_remover.getOutputCol(), outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=hashingTF.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "# # 4. TF-IDF calculation with HashingTF\n",
    "# hashingTF = HashingTF(inputCol=stopwords_remover.getOutputCol(), outputCol=\"rawFeatures\", numFeatures=20)\n",
    "# idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# 4. Chi-square\n",
    "selector = ChiSqSelector(numTopFeatures=2000, featuresCol=idf.getOutputCol(),\n",
    "                         outputCol=\"selectedFeatures\", labelCol=indexer.getOutputCol())\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[indexer, tokenizer, stopwords_remover, hashingTF, idf, selector])\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit the model\n",
    "model = pipeline.fit(df)\n",
    "\n",
    "fit_time = time.time() - start_time\n",
    "print(f'fit_time={fit_time}')\n",
    "\n",
    "# Transform the test data\n",
    "result = model.transform(df)\n",
    "\n",
    "transform_time = time.time() - fit_time - start_time\n",
    "print(f'transform_time={transform_time}')\n",
    "\n",
    "# Extract the CountVectorizer model and ChiSqSelector model from the pipeline\n",
    "cv_model = model.stages[3]\n",
    "selector_model = model.stages[5]\n",
    "\n",
    "# Get the vocabulary\n",
    "vocab = cv_model.vocabulary\n",
    "\n",
    "# Get the top 2000 selected features\n",
    "top2000 = selector_model.selectedFeatures\n",
    "\n",
    "# Map indices to terms and sort them\n",
    "selected_terms = sorted([vocab[index] for index in top2000])\n",
    "    \n",
    "with open(\"output_ds.txt\", \"w\") as f:\n",
    "    for term in selected_terms:\n",
    "        # print(term)\n",
    "        f.write(f\"{term} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901f7be5-f7e2-47e0-b2a2-575f6496cce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(selected_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ab55c9-03f0-4f00-ac18-7ae134fda96f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8085ab1c-d206-4c2b-be82-dc3e76a36942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca47b11e-501d-4ff9-8f5e-08459691bb2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (DIC24)",
   "language": "python",
   "name": "python3_dic24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
