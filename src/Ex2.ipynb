{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import RegexTokenizer, CountVectorizer, StopWordsRemover, HashingTF, IDF, ChiSqSelector, StringIndexer\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "partial_dataset = \"/user/dic24_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "full_dataset = \"/user/dic24_shared/amazon-reviews/full/reviewscombined.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "24/05/24 13:22:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/24 13:22:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/05/24 13:22:46 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/05/24 13:22:46 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "24/05/24 13:22:46 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "24/05/24 13:22:46 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "24/05/24 13:22:46 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "24/05/24 13:22:46 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "24/05/24 13:22:46 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n",
      "24/05/24 13:22:46 WARN Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.\n",
      "24/05/24 13:22:48 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File read successfully.\n",
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- helpful: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      "\n",
      "+----------+--------------------+-------+-------+--------------------+-----------+--------------+--------------------+-------------------+--------------+\n",
      "|      asin|            category|helpful|overall|          reviewText| reviewTime|    reviewerID|        reviewerName|            summary|unixReviewTime|\n",
      "+----------+--------------------+-------+-------+--------------------+-----------+--------------+--------------------+-------------------+--------------+\n",
      "|0981850006|Patio_Lawn_and_Garde| [6, 7]|    5.0|This was a gift f...| 12 3, 2009|A2VNYWOPJ13AFP|Amazon Customer \"...|             Delish|    1259798400|\n",
      "|B00002N66D|Patio_Lawn_and_Garde| [1, 1]|    5.0|This is a very ni...| 12 3, 2012|A2E5XXXC07AGA7|               James|      Nice spreader|    1354492800|\n",
      "|B00002N67U|Patio_Lawn_and_Garde| [0, 1]|    1.0|The metal base wi...|08 13, 2008|A16PX63WZIEQ13|             Finaldx|Terrible spike base|    1218585600|\n",
      "|B00002N6AN|Patio_Lawn_and_Garde| [0, 0]|    4.0|For the most part...| 10 1, 2009|A2OSWM3522VARA|Wayne Allen \"Moto...|  gets the job done|    1254355200|\n",
      "|B00002N8K3|Patio_Lawn_and_Garde| [4, 5]|    1.0|This hose is supp...|07 13, 2013|A2SX9YPPGEUADI|HappyCamper \"Happ...|          The worst|    1373673600|\n",
      "+----------+--------------------+-------+-------+--------------------+-----------+--------------+--------------------+-------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TFIDF_ChiSq_Selection\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "try:\n",
    "    df = spark.read.json(partial_dataset)\n",
    "    print(\"File read successfully.\")\n",
    "    \n",
    "    # Show the schema and some data\n",
    "    df.printSchema()\n",
    "    df.show(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Tokenization\n",
    "tokenizer = RegexTokenizer(inputCol=\"reviewText\", outputCol=\"words\", pattern=r'\\s+|\\t+|\\d+|[(){}.!?,;:+=-_\"\\`~#@&*%€$§\\\\/]+', gaps=True)\n",
    "tokenized = tokenizer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Case Folding and Stopword Removal\n",
    "# Convert all text to lower case\n",
    "tokenized = tokenized.withColumn(\"words\", col(\"words\").cast(\"array<string>\").alias(\"words\"))\n",
    "\n",
    "# Remove stopwords\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "filtered = stopwords_remover.transform(tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Step 3: TF-IDF Calculation\n",
    "vectorizer = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"rawFeatures\", vocabSize=20000)\n",
    "vectorizer_model = vectorizer.fit(filtered)\n",
    "featurizedData = vectorizer_model.transform(filtered)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Step 4: Convert Category to Numeric\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "indexedData = indexer.fit(rescaledData).transform(rescaledData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate top 75 discriminative terms for each category\n",
    "def get_top_terms_for_category(category_index):\n",
    "    category_data = indexedData.filter(indexedData.categoryIndex == category_index)\n",
    "    selector = ChiSqSelector(numTopFeatures=75, featuresCol=\"features\", outputCol=\"selectedFeatures\", labelCol=\"categoryIndex\")\n",
    "    model = selector.fit(category_data)\n",
    "\n",
    "    selected_features = model.selectedFeatures\n",
    "    vocabulary = vectorizer_model.vocabulary\n",
    "    # chi_square_values = model.transform(category_data).select(\"selectedFeatures\").first().selectedFeatures\n",
    "    selected_terms = [(vocabulary[idx], chi_square_values[idx]) for idx in selected_features]\n",
    "    selected_terms.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return selected_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "categories = df.select(\"category\").distinct().collect()\n",
    "category_index_map = {row[\"category\"]: idx for idx, row in enumerate(sorted(categories, key=lambda x: x[\"category\"]))}\n",
    "\n",
    "# Generate output for each category\n",
    "category_output = []\n",
    "\n",
    "for category, index in category_index_map.items():\n",
    "    top_terms = get_top_terms_for_category(index)\n",
    "    category_terms = f\"{category} \" + \" \".join([f\"{term}:{value}\" for term, value in top_terms])\n",
    "    category_output.append(category_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output_ds.txt\", \"w\") as f:\n",
    "    for line in sorted(category_output):\n",
    "        f.write(f\"{line}\\n\")\n",
    "\n",
    "    # Generate merged dictionary\n",
    "    all_terms = sorted(set(term.split(':')[0] for line in category_output for term in line.split()[1:]))\n",
    "    f.write(\" \".join(all_terms) + \"\\n\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (DIC24)",
   "language": "python",
   "name": "python3_dic24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
