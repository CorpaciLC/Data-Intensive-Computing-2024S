{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import RegexTokenizer, CountVectorizer, StopWordsRemover, HashingTF, IDF, ChiSqSelector\n",
    "from pyspark.ml.feature import Tokenizer, Normalizer, StringIndexer\n",
    "from pyspark.sql.functions import col, lower\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "from pyspark.mllib.util import MLUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_dataset = \"/user/dic24_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "full_dataset = \"/user/dic24_shared/amazon-reviews/full/reviewscombined.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TFIDF_ChiSq_Selection\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "try:\n",
    "    df = spark.read.json(partial_dataset)\n",
    "    print(\"File read successfully.\")\n",
    "    \n",
    "    # Show the schema and some data\n",
    "    df.printSchema()\n",
    "    df.show(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: RDDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Datasets/DataFrames: Spark ML and Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# casefolding\n",
    "df = df.withColumn(\"reviewText\", lower(col(\"reviewText\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert category to numeric\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "\n",
    "#Creating a pipeline:\n",
    "\n",
    "# 1. tokenize\n",
    "tokenizer = RegexTokenizer(inputCol=\"reviewText\", outputCol=\"words\", pattern=r'\\s+|\\t+|\\d+|[(){}.!?,;:+=-_\"\\`~#@&*%€$§\\\\/]+', gaps=True)\n",
    "\n",
    "# 3. stopwords removal\n",
    "stopwords_remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"filtered_words\")\n",
    "\n",
    "\n",
    "# 4. TF-IDF calculation with HashingTF\n",
    "hashingTF = HashingTF(inputCol=stopwords_remover.getOutputCol(), outputCol=\"rawFeatures\", numFeatures=20)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# Chi-square\n",
    "selector = ChiSqSelector(numTopFeatures=2000, featuresCol=hashingTF.getOutputCol(),\n",
    "                         outputCol=\"selectedFeatures\", labelCol=indexer.getOutputCol())\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, stopwords_remover, hashingTF, idf, selector])\n",
    "\n",
    "# Training the pipeline\n",
    "model = pipeline.fit(df)\n",
    "result = model.transform(df)\n",
    "\n",
    "# Writing the result to a file\n",
    "terms = result.select(\"selectedFeatures\").collect()\n",
    "with open(\"output_ds.txt\", \"w\") as f:\n",
    "    for term in terms:\n",
    "        f.write(f\"{term}\\n\")\n",
    "\n",
    "# Stopping Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Spark\n",
    "spark = SparkSession.builder.appName(\"Part 3 Classification\").getOrCreate()\n",
    "\n",
    "# Loading data\n",
    "data = spark.read.json(\"/path/to/your/data.json\")\n",
    "\n",
    "# Creating the pipeline (adding SVM)\n",
    "normalizer = Normalizer(inputCol=\"selectedFeatures\", outputCol=\"normFeatures\")\n",
    "svm = LinearSVC(featuresCol=\"normFeatures\", labelCol=\"category\")\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, selector, normalizer, svm])\n",
    "\n",
    "# Splitting the data\n",
    "# split in train, test, validation\n",
    "train_df, test_df, validation_df = df.randomSplit([0.7, 0.15, 0.15], seed=42)\n",
    "\n",
    "\n",
    "# Show the count of each set\n",
    "print(\"Training set count:\", train_df.count())\n",
    "print(\"Test set count:\", test_df.count())\n",
    "print(\"Validation set count:\", validation_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(selector.numTopFeatures, [2000, 500])  # number of features selected\n",
    "             .addGrid(svm.regParam, [0.01, 0.1, 1.0])  # Regularization parameter\n",
    "             .addGrid(svm.maxIter, [10, 50])  # Maximum number of iterations\n",
    "             .build())\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(labelCol=indexer.getOutputCol(), metricName=\"f1\"),\n",
    "                          numFolds=3)\n",
    "\n",
    "# Fit the pipeline to training texts.\n",
    "model = crossval.fit(train_df)\n",
    "\n",
    "# Evaluating the model\n",
    "predictions = model.transform(test_df)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"category\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"F1 Score: {f1_score}\")\n",
    "\n",
    "# Stopping Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
