{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f332df40-a2c1-4774-bf1a-08b760c9a67d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import RegexTokenizer, CountVectorizer, StopWordsRemover, HashingTF, IDF, ChiSqSelector, CountVectorizer\n",
    "from pyspark.ml.feature import Tokenizer, Normalizer, StringIndexer\n",
    "from pyspark.sql.functions import col, lower\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd200b63-8ca2-4222-817e-37d5ea6fa35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_dataset = \"/user/dic24_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "full_dataset = \"/user/dic24_shared/amazon-reviews/full/reviewscombined.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "035eca20-70d1-4a8f-9a00-47ff9624a6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File read successfully.\n",
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- helpful: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      "\n",
      "+----------+--------------------+-------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
      "|      asin|            category|helpful|overall|          reviewText| reviewTime|    reviewerID|        reviewerName|             summary|unixReviewTime|\n",
      "+----------+--------------------+-------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
      "|0981850006|Patio_Lawn_and_Garde| [6, 7]|    5.0|This was a gift f...| 12 3, 2009|A2VNYWOPJ13AFP|Amazon Customer \"...|              Delish|    1259798400|\n",
      "|0981850006|Patio_Lawn_and_Garde| [0, 0]|    5.0|My husband rarely...|06 13, 2013|A20DWVV8HML3AW|               Cyndy|Just what my husb...|    1371081600|\n",
      "|0981850006|Patio_Lawn_and_Garde|[9, 11]|    5.0|This guy knows hi...|11 12, 2009|A3RVP3YBYYOPRH|         John Warren|     Love This Show!|    1257984000|\n",
      "|0981850006|Patio_Lawn_and_Garde| [1, 1]|    5.0|THIS CAN BE FOUND...|08 24, 2011|A28XY55TP3Q90O|JSG \"Lover of Lea...|BUY ELSEWHERE, TO...|    1314144000|\n",
      "|0981850006|Patio_Lawn_and_Garde| [2, 2]|    5.0|Quite good partic...|06 17, 2011|A3VZW1BGUQO0V3|Marc G. Belisle \"...|Quite good but I ...|    1308268800|\n",
      "+----------+--------------------+-------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Text_Classification_all\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "try:\n",
    "    df = spark.read.json(full_dataset)\n",
    "    print(\"File read successfully.\")\n",
    "    \n",
    "    # Show the schema and some data\n",
    "    df.printSchema()\n",
    "    df.show(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c336398-2c4d-4622-a71f-c8693594b730",
   "metadata": {},
   "source": [
    "# Part 1: RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8824c96-9b4e-4f26-9670-3a9ba0aaa784",
   "metadata": {},
   "source": [
    "# Part 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10c26a83-efb3-4811-9efb-5ede4f8209eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casefolding\n",
    "df = df.withColumn(\"reviewText\", lower(col(\"reviewText\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb22f5e2-c855-454c-acf1-49673b463608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сonverting category to numeric\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "\n",
    "# Creating the pipeline:\n",
    "\n",
    "# 1. tokenization\n",
    "tokenizer = RegexTokenizer(inputCol=\"reviewText\", outputCol=\"words\", pattern=r'\\s+|\\t+|\\d+|[(){}.!?,;:+=-_\"\\`~#@&*%€$§\\\\/]+', gaps=True)\n",
    "\n",
    "# 3. Stopwords removal\n",
    "stopwords_remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"filtered_words\")\n",
    "\n",
    "# 4. tf-idf calculation with CountVectorizer\n",
    "hashingTF = CountVectorizer(inputCol=stopwords_remover.getOutputCol(), outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=hashingTF.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "# # 4. TF-IDF calculation with HashingTF\n",
    "# hashingTF = HashingTF(inputCol=stopwords_remover.getOutputCol(), outputCol=\"rawFeatures\", numFeatures=20)\n",
    "# idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# 5. Chi-square\n",
    "selector = ChiSqSelector(numTopFeatures=2000, featuresCol=idf.getOutputCol(),\n",
    "                         outputCol=\"selectedFeatures\", labelCol=indexer.getOutputCol())\n",
    "\n",
    "\n",
    "# pipeline = Pipeline(stages=[indexer, tokenizer, stopwords_remover, hashingTF, idf, selector])\n",
    "\n",
    "\n",
    "# start_time = time.time()\n",
    "# model = pipeline.fit(df)\n",
    "# fit_time = time.time() - start_time\n",
    "# print(f'fit_time={fit_time}')\n",
    "\n",
    "# # Transform the test data\n",
    "# result = model.transform(df)\n",
    "# transform_time = time.time() - fit_time - start_time\n",
    "# print(f'transform_time={transform_time}')\n",
    "\n",
    "# # Extract the CountVectorizer model from the pipeline\n",
    "# count_vectorizer_model = model.stages[3]\n",
    "# vocab = count_vectorizer_model.vocabulary\n",
    "\n",
    "# # Get the selected feature indices\n",
    "# selected_indices = result.select(\"selectedFeatures\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# # Convert the indices to terms\n",
    "# selected_terms = [vocab[index] for indices in selected_indices for index in indices.indices]\n",
    "\n",
    "    \n",
    "# with open(\"output_ds.txt\", \"w\") as f:\n",
    "#     for term in selected_terms:\n",
    "#         # print(term)\n",
    "#         f.write(f\"{term} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41ba8cf-5ef3-4c40-be27-29e31d3bde95",
   "metadata": {},
   "source": [
    "# Part 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f299317-66ab-445e-b252-1fa1900bb0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Normalizing\n",
    "normalizer = Normalizer(inputCol=selector.getOutputCol(), outputCol=\"normFeatures\")\n",
    "\n",
    "# 7. Creating SVM classificator\n",
    "svm = LinearSVC(featuresCol=normalizer.getOutputCol(), labelCol=\"categoryIndex\")\n",
    "\n",
    "# 8. Using One-vs-Rest for multiclass classification\n",
    "ovr = OneVsRest(classifier=svm, labelCol=\"categoryIndex\")\n",
    "\n",
    "\n",
    "# pipeline = Pipeline(stages=[indexer, tokenizer, stopwords_remover, hashingTF, idf, selector, normalizer, ovr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6189059-fdd8-4077-ae6b-30c7cfa737c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "train_df, validation_df, test_df,  = df.randomSplit([0.7, 0.15, 0.15], seed=42)\n",
    "\n",
    "# train_df = train_df.sample(0.01)\n",
    "# validation_df = train_df.sample(0.05)\n",
    "# test_df = train_df.sample(0.05)\n",
    "\n",
    "# Show the count of each set\n",
    "print(\"Training set count:\", train_df.count())\n",
    "print(\"Test set count:\", test_df.count())\n",
    "print(\"Validation set count:\", validation_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43315cfc-0489-4d98-9cdc-7c3eeba8c35b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Creating a grid of parameters\n",
    "# paramGrid = (ParamGridBuilder()\n",
    "#              .addGrid(selector.numTopFeatures, [2000, 500])  # number of features selected\n",
    "#              .addGrid(svm.regParam, [0.01, 0.1, 1.0])  # Regularization parameter\n",
    "#              .addGrid(svm.maxIter, [10, 50])  # Maximum number of iterations\n",
    "#              .addGrid(svm.standardization, [True, False]) # Standardization\n",
    "#              .build())\n",
    "\n",
    "\n",
    "# # Creating a Cross Validator for Grid search\n",
    "# crossval = CrossValidator(estimator=pipeline,\n",
    "#                           estimatorParamMaps=paramGrid,\n",
    "#                           evaluator=MulticlassClassificationEvaluator(labelCol=\"categoryIndex\", metricName=\"f1\"),\n",
    "#                           numFolds=3)\n",
    "\n",
    "# # Model training\n",
    "# try:\n",
    "#     print(\"Starting model training...\")\n",
    "#     model = crossval.fit(train_df)\n",
    "#     print(\"Model training completed.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error during model training: {e}\")\n",
    "\n",
    "\n",
    "# # Predictions on the test data\n",
    "# try:\n",
    "#     print(\"Making predictions...\")\n",
    "#     predictions = model.transform(test_df)\n",
    "#     print(\"Predictions made.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error during predictions: {e}\")\n",
    "\n",
    "\n",
    "# # Evaluating the model\n",
    "# try:\n",
    "#     print(\"Evaluating the model...\")\n",
    "#     evaluator = MulticlassClassificationEvaluator(labelCol=\"categoryIndex\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "#     f1_score = evaluator.evaluate(predictions)\n",
    "#     print(f\"F1 Score: {f1_score}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error during evaluation: {e}\")\n",
    "\n",
    "\n",
    "# # Stopping Spark\n",
    "# print(\"Stopping Spark...\")\n",
    "# spark.stop()\n",
    "# print(\"Spark stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2465b52b-cedb-4641-94d0-65682b281fdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # # Create pipeline\n",
    "# # pipeline = Pipeline(stages=[indexer, tokenizer, stopwords_remover, hashingTF, idf, selector, normalizer, ovr])\n",
    "\n",
    "# # # Create parameter grid\n",
    "# # paramGrid = (ParamGridBuilder()\n",
    "# #              .addGrid(selector.numTopFeatures, [50, 2000])\n",
    "# #              .addGrid(svm.regParam, [0.01, 0.1, 1.0])\n",
    "# #              .addGrid(svm.maxIter, [10, 50])\n",
    "# #              .addGrid(svm.standardization, [True, False])\n",
    "# #              .build())\n",
    "\n",
    "# # Define evaluator\n",
    "\n",
    "# # Precompute stages up to IDF\n",
    "# pre_pipeline = Pipeline(stages=[indexer, tokenizer, stopwords_remover, count_vectorizer, idf])\n",
    "\n",
    "# # Fit pre_pipeline\n",
    "# pre_model = pre_pipeline.fit(train_df)\n",
    "\n",
    "# # Cache the precomputed results\n",
    "# train_df = pre_model.transform(train_df).cache()\n",
    "# test_df = pre_model.transform(test_df).cache()\n",
    "# validation_df = pre_model.transform(validation_df).cache()\n",
    "\n",
    "\n",
    "\n",
    "# evaluator = MulticlassClassificationEvaluator(labelCol=\"categoryIndex\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "# best_model = None\n",
    "# best_f1_score = float('-inf')\n",
    "# best_params = None\n",
    "\n",
    "\n",
    "# numTopFeatures = [500, 2000]\n",
    "# regParams =  [0.01, 0.1, 1.0]\n",
    "# maxIters = [10, 50]\n",
    "# standardizations = [True, False]\n",
    "\n",
    "# # Iterate over the parameter grid\n",
    "# for numTopFeature in numTopFeatures:\n",
    "#     for regParam in regParams:\n",
    "#         for maxIter in maxIters:\n",
    "#             for standardization in standardizations:\n",
    "#                 print(f'numTopFeature={numTopFeature}, regParam={regParam}, maxIter={maxIter}, standardization={standardization}')\n",
    "\n",
    "#                 selector = ChiSqSelector(numTopFeatures=numTopFeature, featuresCol=idf.getOutputCol(),\n",
    "#                                      outputCol=\"selectedFeatures\", labelCol=indexer.getOutputCol())\n",
    "#                 svm = LinearSVC(featuresCol=normalizer.getOutputCol(), labelCol=\"categoryIndex\", regParam=regParam, maxIter=maxIter, standardization=standardization)\n",
    "\n",
    "#                 pipeline = Pipeline(stages=[indexer, tokenizer, stopwords_remover, hashingTF, idf, selector, normalizer, ovr])\n",
    "\n",
    "#                 # Create a new pipeline model with the current parameter combination\n",
    "#                 start_time = time.time()\n",
    "#                 print('fit')\n",
    "#                 pipeline_model = pipeline.fit(train_df)\n",
    "#                 fit_time = time.time() - start_time\n",
    "#                 print(f'fit_time={fit_time}')\n",
    "\n",
    "#                 print('transform')\n",
    "#                 # Make predictions on the validation set\n",
    "#                 predictions = pipeline_model.transform(validation_df)\n",
    "#                 transform_time = time.time() - start_time  - fit_time\n",
    "#                 print(f'transform_time={transform_time}')\n",
    "\n",
    "#                 print('evaluate')\n",
    "#                 # Evaluate the model\n",
    "#                 f1_score = evaluator.evaluate(predictions)\n",
    "#                 evaluate_time = time.time() - start_time  - fit_time - transform_time\n",
    "#                 print(f'fit_time={evaluate_time}')\n",
    "\n",
    "#                 print(f'f1_score={f1_score}')\n",
    "                \n",
    "#                 if f1_score > best_f1_score:\n",
    "#                     best_f1_score = f1_score\n",
    "#                     best_model = pipeline_model\n",
    "#                     best_params = f'numTopFeature={numTopFeature}, regParam={regParam}, maxIter={maxIter}, standardization={standardization}'\n",
    "\n",
    "# print(f\"Best F1 Score: {best_f1_score}\")\n",
    "# print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "# # Predictions on the test data\n",
    "# try:\n",
    "#     print(\"Making predictions...\")\n",
    "#     predictions = best_model.transform(test_df)\n",
    "#     print(\"Predictions made.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error during predictions: {e}\")\n",
    "\n",
    "# # Evaluating the model\n",
    "# try:\n",
    "#     print(\"Evaluating the model...\")\n",
    "#     f1_score = evaluator.evaluate(predictions)\n",
    "#     print(f\"F1 Score: {f1_score}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error during evaluation: {e}\")\n",
    "\n",
    "# # Stopping Spark\n",
    "# print(\"Stopping Spark...\")\n",
    "# spark.stop()\n",
    "# print(\"Spark stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea582a54-03b9-4320-8560-92f6870dc1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = ChiSqSelector(numTopFeatures=500, featuresCol=idf.getOutputCol(),\n",
    "                         outputCol=\"selectedFeatures\", labelCol=indexer.getOutputCol())\n",
    "\n",
    "pre_pipeline = Pipeline(stages=[indexer, tokenizer, stopwords_remover, hashingTF, idf, selector])\n",
    "# Fit pre_pipeline\n",
    "pre_model = pre_pipeline.fit(train_df)\n",
    "\n",
    "# Cache the precomputed results\n",
    "preprocessed_train_df_500 = pre_model.transform(train_df).cache()\n",
    "preprocessed_test_df_500 = pre_model.transform(test_df).cache()\n",
    "preprocessed_validation_df_500 = pre_model.transform(validation_df).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f6d2f1-7487-48c2-aa89-b7cc18635093",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = ChiSqSelector(numTopFeatures=2000, featuresCol=idf.getOutputCol(),\n",
    "                         outputCol=\"selectedFeatures\", labelCol=indexer.getOutputCol())\n",
    "\n",
    "pre_pipeline = Pipeline(stages=[indexer, tokenizer, stopwords_remover, hashingTF, idf, selector])\n",
    "# Fit pre_pipeline\n",
    "pre_model = pre_pipeline.fit(train_df)\n",
    "\n",
    "# Cache the precomputed results\n",
    "preprocessed_train_df_2000 = pre_model.transform(train_df).cache()\n",
    "preprocessed_test_df_2000 = pre_model.transform(test_df).cache()\n",
    "preprocessed_validation_df_2000 = pre_model.transform(validation_df).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85eb966-05e8-4047-a5b6-734f4749d2a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, IDF, ChiSqSelector, StringIndexer, Normalizer\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "import time\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a5f6c7-f8ad-4ddc-8e5d-eedf8e58e6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(LinearSVC.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(LinearSVC.maxIter, [10, 50]) \\\n",
    "    .addGrid(LinearSVC.standardization, [True, False]) \\\n",
    "    .build()\n",
    "\n",
    "\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"categoryIndex\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "best_model = None\n",
    "best_f1_score = float('-inf')\n",
    "best_params = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fed7f7c-3873-4eab-a221-893508ce6adf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = preprocessed_train_df_500\n",
    "validation_df = preprocessed_validation_df_500\n",
    "test_df = preprocessed_test_df_500\n",
    "\n",
    "\n",
    "# Iterate over the parameter grid\n",
    "for params in paramGrid:\n",
    "    regParam = params[LinearSVC.regParam]\n",
    "    maxIter = params[LinearSVC.maxIter]\n",
    "    standardization = params[LinearSVC.standardization]\n",
    "\n",
    "    print(f'fregParam={regParam}, maxIter={maxIter}, standardization={standardization}')\n",
    "\n",
    "    normalizer = Normalizer(inputCol=\"selectedFeatures\", outputCol=\"normFeatures\")\n",
    "    svm = LinearSVC(featuresCol=\"normFeatures\", labelCol=\"categoryIndex\", regParam=regParam, maxIter=maxIter, standardization=standardization)\n",
    "    ovr = OneVsRest(classifier=svm, labelCol=\"categoryIndex\")\n",
    "\n",
    "    pipeline = Pipeline(stages=[normalizer, ovr])\n",
    "\n",
    "    # Create a new pipeline model with the current parameter combination\n",
    "    start_time = time.time()\n",
    "    pipeline_model = pipeline.fit(train_df)\n",
    "    fit_time = time.time() - start_time\n",
    "    print(f'fit_time={fit_time}')\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    predictions = pipeline_model.transform(validation_df)\n",
    "    transform_time = time.time() - start_time - fit_time\n",
    "    print(f'transform_time={transform_time}')\n",
    "\n",
    "    # Evaluate the model\n",
    "    f1_score = evaluator.evaluate(predictions)\n",
    "    evaluate_time = time.time() - start_time - fit_time - transform_time\n",
    "    print(f'evaluate_time={evaluate_time}')\n",
    "\n",
    "    print(f'f1_score={f1_score}')\n",
    "\n",
    "    if f1_score > best_f1_score:\n",
    "        best_f1_score = f1_score\n",
    "        best_model = pipeline_model\n",
    "        best_params = f'regParam={regParam}, maxIter={maxIter}, standardization={standardization}'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901f7be5-f7e2-47e0-b2a2-575f6496cce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = preprocessed_train_df_2000\n",
    "validation_df = preprocessed_validation_df_2000\n",
    "test_df = preprocessed_test_df_2000\n",
    "\n",
    "\n",
    "# Iterate over the parameter grid\n",
    "for params in paramGrid:\n",
    "    regParam = params[LinearSVC.regParam]\n",
    "    maxIter = params[LinearSVC.maxIter]\n",
    "    standardization = params[LinearSVC.standardization]\n",
    "\n",
    "    print(f'fregParam={regParam}, maxIter={maxIter}, standardization={standardization}')\n",
    "\n",
    "    normalizer = Normalizer(inputCol=\"selectedFeatures\", outputCol=\"normFeatures\")\n",
    "    svm = LinearSVC(featuresCol=\"normFeatures\", labelCol=\"categoryIndex\", regParam=regParam, maxIter=maxIter, standardization=standardization)\n",
    "    ovr = OneVsRest(classifier=svm, labelCol=\"categoryIndex\")\n",
    "\n",
    "    pipeline = Pipeline(stages=[normalizer, ovr])\n",
    "\n",
    "    # Create a new pipeline model with the current parameter combination\n",
    "    start_time = time.time()\n",
    "    pipeline_model = pipeline.fit(train_df)\n",
    "    fit_time = time.time() - start_time\n",
    "    print(f'fit_time={fit_time}')\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    predictions = pipeline_model.transform(validation_df)\n",
    "    transform_time = time.time() - start_time - fit_time\n",
    "    print(f'transform_time={transform_time}')\n",
    "\n",
    "    # Evaluate the model\n",
    "    f1_score = evaluator.evaluate(predictions)\n",
    "    evaluate_time = time.time() - start_time - fit_time - transform_time\n",
    "    print(f'evaluate_time={evaluate_time}')\n",
    "\n",
    "    print(f'f1_score={f1_score}')\n",
    "\n",
    "    if f1_score > best_f1_score:\n",
    "        best_f1_score = f1_score\n",
    "        best_model = pipeline_model\n",
    "        best_params = f'regParam={regParam}, maxIter={maxIter}, standardization={standardization}'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e6acd1-12ab-4937-b86a-c333ef1177ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best F1 Score: {best_f1_score}\")\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "# Predictions on the test data\n",
    "try:\n",
    "    print(\"Making predictions...\")\n",
    "    predictions = best_model.transform(test_df)\n",
    "    print(\"Predictions made.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during predictions: {e}\")\n",
    "\n",
    "# Evaluating the model\n",
    "try:\n",
    "    print(\"Evaluating the model...\")\n",
    "    f1_score = evaluator.evaluate(predictions)\n",
    "    print(f\"F1 Score: {f1_score}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during evaluation: {e}\")\n",
    "\n",
    "# Stopping Spark\n",
    "print(\"Stopping Spark...\")\n",
    "spark.stop()\n",
    "print(\"Spark stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9e40c6-58a9-4cf6-8c6c-ae76d9247e44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19c9149-dfe6-476f-afd2-9002061ef7d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3447871-1874-447c-b952-7a3ba76de9ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ab55c9-03f0-4f00-ac18-7ae134fda96f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (DIC24)",
   "language": "python",
   "name": "python3_dic24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
