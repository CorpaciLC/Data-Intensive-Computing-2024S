{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f48c682-bf56-4d47-b6ca-0217c7c49e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import RegexTokenizer, CountVectorizer, StopWordsRemover, HashingTF, IDF, ChiSqSelector\n",
    "from pyspark.ml.feature import Tokenizer, Normalizer, StringIndexer\n",
    "from pyspark.sql.functions import col, lower\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "from pyspark.mllib.util import MLUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edabdac6-5661-4f82-adba-529e2dccc934",
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_dataset = \"/user/dic24_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "full_dataset = \"/user/dic24_shared/amazon-reviews/full/reviewscombined.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfe3b94a-5b55-4792-8a21-e4abf86f9687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "24/05/24 14:54:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/24 14:54:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/05/24 14:54:27 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/05/24 14:54:27 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "24/05/24 14:54:27 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "24/05/24 14:54:27 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "24/05/24 14:54:30 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File read successfully.\n",
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- helpful: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      "\n",
      "+----------+--------------------+-------+-------+--------------------+-----------+--------------+--------------------+-------------------+--------------+\n",
      "|      asin|            category|helpful|overall|          reviewText| reviewTime|    reviewerID|        reviewerName|            summary|unixReviewTime|\n",
      "+----------+--------------------+-------+-------+--------------------+-----------+--------------+--------------------+-------------------+--------------+\n",
      "|0981850006|Patio_Lawn_and_Garde| [6, 7]|    5.0|This was a gift f...| 12 3, 2009|A2VNYWOPJ13AFP|Amazon Customer \"...|             Delish|    1259798400|\n",
      "|B00002N66D|Patio_Lawn_and_Garde| [1, 1]|    5.0|This is a very ni...| 12 3, 2012|A2E5XXXC07AGA7|               James|      Nice spreader|    1354492800|\n",
      "|B00002N67U|Patio_Lawn_and_Garde| [0, 1]|    1.0|The metal base wi...|08 13, 2008|A16PX63WZIEQ13|             Finaldx|Terrible spike base|    1218585600|\n",
      "|B00002N6AN|Patio_Lawn_and_Garde| [0, 0]|    4.0|For the most part...| 10 1, 2009|A2OSWM3522VARA|Wayne Allen \"Moto...|  gets the job done|    1254355200|\n",
      "|B00002N8K3|Patio_Lawn_and_Garde| [4, 5]|    1.0|This hose is supp...|07 13, 2013|A2SX9YPPGEUADI|HappyCamper \"Happ...|          The worst|    1373673600|\n",
      "+----------+--------------------+-------+-------+--------------------+-----------+--------------+--------------------+-------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TFIDF_ChiSq_Selection\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "try:\n",
    "    df = spark.read.json(partial_dataset)\n",
    "    print(\"File read successfully.\")\n",
    "    \n",
    "    # Show the schema and some data\n",
    "    df.printSchema()\n",
    "    df.show(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c0a6459-6438-417e-bfd7-6e3f4a8e7e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------+-------+--------------------+-----------+--------------+--------------------+-------------------+--------------+\n",
      "|      asin|            category|helpful|overall|          reviewText| reviewTime|    reviewerID|        reviewerName|            summary|unixReviewTime|\n",
      "+----------+--------------------+-------+-------+--------------------+-----------+--------------+--------------------+-------------------+--------------+\n",
      "|0981850006|Patio_Lawn_and_Garde| [6, 7]|    5.0|this was a gift f...| 12 3, 2009|A2VNYWOPJ13AFP|Amazon Customer \"...|             Delish|    1259798400|\n",
      "|B00002N66D|Patio_Lawn_and_Garde| [1, 1]|    5.0|this is a very ni...| 12 3, 2012|A2E5XXXC07AGA7|               James|      Nice spreader|    1354492800|\n",
      "|B00002N67U|Patio_Lawn_and_Garde| [0, 1]|    1.0|the metal base wi...|08 13, 2008|A16PX63WZIEQ13|             Finaldx|Terrible spike base|    1218585600|\n",
      "|B00002N6AN|Patio_Lawn_and_Garde| [0, 0]|    4.0|for the most part...| 10 1, 2009|A2OSWM3522VARA|Wayne Allen \"Moto...|  gets the job done|    1254355200|\n",
      "|B00002N8K3|Patio_Lawn_and_Garde| [4, 5]|    1.0|this hose is supp...|07 13, 2013|A2SX9YPPGEUADI|HappyCamper \"Happ...|          The worst|    1373673600|\n",
      "+----------+--------------------+-------+-------+--------------------+-----------+--------------+--------------------+-------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# casefolding\n",
    "df = df.withColumn(\"reviewText\", lower(col(\"reviewText\")))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e35af735-fef1-43f3-a068-d41507318aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set count: 55401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set count: 11621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set count: 11807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_df, test_df, validation_df = df.randomSplit([0.7, 0.15, 0.15], seed=42)\n",
    "\n",
    "\n",
    "# Show the count of each set\n",
    "print(\"Training set count:\", train_df.count())\n",
    "print(\"Test set count:\", test_df.count())\n",
    "print(\"Validation set count:\", validation_df.count())\n",
    "# Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "# test = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3656aaa0-faa5-49a4-b4de-8c9653b41613",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve 'normFeatures' given input columns: [asin, category, categoryIndex, filtered_words, helpful, overall, rawFeatures, reviewText, reviewTime, reviewerID, reviewerName, selectedFeatures, summary, unixReviewTime, words];\n'Project [categoryIndex#1018, 'normFeatures]\n+- Project [asin#7, category#8, helpful#9, overall#10, reviewText#89, reviewTime#12, reviewerID#13, reviewerName#14, summary#15, unixReviewTime#16L, categoryIndex#1018, words#1036, filtered_words#1055, rawFeatures#1074, UDF(rawFeatures#1074) AS selectedFeatures#1146]\n   +- Project [asin#7, category#8, helpful#9, overall#10, reviewText#89, reviewTime#12, reviewerID#13, reviewerName#14, summary#15, unixReviewTime#16L, categoryIndex#1018, words#1036, filtered_words#1055, UDF(filtered_words#1055) AS rawFeatures#1074]\n      +- Project [asin#7, category#8, helpful#9, overall#10, reviewText#89, reviewTime#12, reviewerID#13, reviewerName#14, summary#15, unixReviewTime#16L, categoryIndex#1018, words#1036, UDF(words#1036) AS filtered_words#1055]\n         +- Project [asin#7, category#8, helpful#9, overall#10, reviewText#89, reviewTime#12, reviewerID#13, reviewerName#14, summary#15, unixReviewTime#16L, categoryIndex#1018, UDF(reviewText#89) AS words#1036]\n            +- Project [asin#7, category#8, helpful#9, overall#10, reviewText#89, reviewTime#12, reviewerID#13, reviewerName#14, summary#15, unixReviewTime#16L, UDF(cast(category#8 as string)) AS categoryIndex#1018]\n               +- Sample 0.0, 0.7, false, 42\n                  +- Sort [asin#7 ASC NULLS FIRST, category#8 ASC NULLS FIRST, helpful#9 ASC NULLS FIRST, overall#10 ASC NULLS FIRST, reviewText#89 ASC NULLS FIRST, reviewTime#12 ASC NULLS FIRST, reviewerID#13 ASC NULLS FIRST, reviewerName#14 ASC NULLS FIRST, summary#15 ASC NULLS FIRST, unixReviewTime#16L ASC NULLS FIRST], false\n                     +- Project [asin#7, category#8, helpful#9, overall#10, lower(reviewText#78) AS reviewText#89, reviewTime#12, reviewerID#13, reviewerName#14, summary#15, unixReviewTime#16L]\n                        +- Project [asin#7, category#8, helpful#9, overall#10, lower(reviewText#11) AS reviewText#78, reviewTime#12, reviewerID#13, reviewerName#14, summary#15, unixReviewTime#16L]\n                           +- Relation [asin#7,category#8,helpful#9,overall#10,reviewText#11,reviewTime#12,reviewerID#13,reviewerName#14,summary#15,unixReviewTime#16L] json\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 47\u001b[0m\n\u001b[1;32m     35\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(stages\u001b[38;5;241m=\u001b[39m[indexer, tokenizer, stopwords_remover, hashingTF, selector, ovr])\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# # ParamGrid for Cross Validation\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# paramGrid = (ParamGridBuilder()\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#              .addGrid(selector.numTopFeatures, [2000, 500])  # number of features selected\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m \n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Fit the pipeline to training documents.\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Make predictions on test documents and print columns of interest.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(test_df)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/pipeline.py:114\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    112\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/classification.py:2903\u001b[0m, in \u001b[0;36mOneVsRest._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m   2901\u001b[0m     multiclassLabeled \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mselect(labelCol, featuresCol, weightCol)\n\u001b[1;32m   2902\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2903\u001b[0m     multiclassLabeled \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabelCol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeaturesCol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2905\u001b[0m \u001b[38;5;66;03m# persist if underlying dataset is not persistent.\u001b[39;00m\n\u001b[1;32m   2906\u001b[0m handlePersistence \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mstorageLevel \u001b[38;5;241m==\u001b[39m StorageLevel(\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py:1685\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols):\n\u001b[1;32m   1665\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1666\u001b[0m \n\u001b[1;32m   1667\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1683\u001b[0m \u001b[38;5;124;03m    [Row(name='Alice', age=12), Row(name='Bob', age=15)]\u001b[39;00m\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1685\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msql_ctx)\n",
      "File \u001b[0;32m/sw/venv/python39/dic24/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve 'normFeatures' given input columns: [asin, category, categoryIndex, filtered_words, helpful, overall, rawFeatures, reviewText, reviewTime, reviewerID, reviewerName, selectedFeatures, summary, unixReviewTime, words];\n'Project [categoryIndex#1018, 'normFeatures]\n+- Project [asin#7, category#8, helpful#9, overall#10, reviewText#89, reviewTime#12, reviewerID#13, reviewerName#14, summary#15, unixReviewTime#16L, categoryIndex#1018, words#1036, filtered_words#1055, rawFeatures#1074, UDF(rawFeatures#1074) AS selectedFeatures#1146]\n   +- Project [asin#7, category#8, helpful#9, overall#10, reviewText#89, reviewTime#12, reviewerID#13, reviewerName#14, summary#15, unixReviewTime#16L, categoryIndex#1018, words#1036, filtered_words#1055, UDF(filtered_words#1055) AS rawFeatures#1074]\n      +- Project [asin#7, category#8, helpful#9, overall#10, reviewText#89, reviewTime#12, reviewerID#13, reviewerName#14, summary#15, unixReviewTime#16L, categoryIndex#1018, words#1036, UDF(words#1036) AS filtered_words#1055]\n         +- Project [asin#7, category#8, helpful#9, overall#10, reviewText#89, reviewTime#12, reviewerID#13, reviewerName#14, summary#15, unixReviewTime#16L, categoryIndex#1018, UDF(reviewText#89) AS words#1036]\n            +- Project [asin#7, category#8, helpful#9, overall#10, reviewText#89, reviewTime#12, reviewerID#13, reviewerName#14, summary#15, unixReviewTime#16L, UDF(cast(category#8 as string)) AS categoryIndex#1018]\n               +- Sample 0.0, 0.7, false, 42\n                  +- Sort [asin#7 ASC NULLS FIRST, category#8 ASC NULLS FIRST, helpful#9 ASC NULLS FIRST, overall#10 ASC NULLS FIRST, reviewText#89 ASC NULLS FIRST, reviewTime#12 ASC NULLS FIRST, reviewerID#13 ASC NULLS FIRST, reviewerName#14 ASC NULLS FIRST, summary#15 ASC NULLS FIRST, unixReviewTime#16L ASC NULLS FIRST], false\n                     +- Project [asin#7, category#8, helpful#9, overall#10, lower(reviewText#78) AS reviewText#89, reviewTime#12, reviewerID#13, reviewerName#14, summary#15, unixReviewTime#16L]\n                        +- Project [asin#7, category#8, helpful#9, overall#10, lower(reviewText#11) AS reviewText#78, reviewTime#12, reviewerID#13, reviewerName#14, summary#15, unixReviewTime#16L]\n                           +- Relation [asin#7,category#8,helpful#9,overall#10,reviewText#11,reviewTime#12,reviewerID#13,reviewerName#14,summary#15,unixReviewTime#16L] json\n"
     ]
    }
   ],
   "source": [
    "# convert category to numeric\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "\n",
    "# 1. tokenize\n",
    "tokenizer = RegexTokenizer(inputCol=\"reviewText\", outputCol=\"words\", pattern=r'\\s+|\\t+|\\d+|[(){}.!?,;:+=-_\"\\`~#@&*%€$§\\\\/]+', gaps=True)\n",
    "\n",
    "# 3. stopwords removal\n",
    "stopwords_remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"filtered_words\")\n",
    "\n",
    "## 4. tf-idf calculation with CountVectorizer\n",
    "# vectorizer = CountVectorizer(inputCol=stopwords_remover.getOutputCol(), outputCol=\"rawFeatures\", vocabSize=20000)\n",
    "# idf = IDF(inputCol=vectorizer.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "# 4. TF-IDF calculation with HashingTF\n",
    "hashingTF = HashingTF(inputCol=stopwords_remover.getOutputCol(), outputCol=\"rawFeatures\", numFeatures=20)\n",
    "\n",
    "\n",
    "# Chi-square\n",
    "selector = ChiSqSelector(numTopFeatures=2000, featuresCol=hashingTF.getOutputCol(),\n",
    "                         outputCol=\"selectedFeatures\", labelCol=indexer.getOutputCol())\n",
    "\n",
    "# Normalize in L^2\n",
    "# normalizer = Normalizer(p=2.0)\n",
    "# data1 = labels.zip(normalizer.transform(features))\n",
    "\n",
    "\n",
    "# SVM Classifier\n",
    "# svm = LinearSVC(featuresCol=\"selectedFeatures\", labelCol=indexer.getOutputCol(), maxIter=10)\n",
    "svm = LinearSVC(maxIter=10, regParam=0.1)\n",
    "ovr = OneVsRest(classifier=svm, featuresCol=\"normFeatures\", labelCol=indexer.getOutputCol())\n",
    "\n",
    "\n",
    "# Pipeline\n",
    "# pipeline = Pipeline(stages=[indexer, tokenizer, stopwords_remover, hashingTF, selector, normalizer, svm])\n",
    "pipeline = Pipeline(stages=[indexer, tokenizer, stopwords_remover, hashingTF, selector, ovr])\n",
    "\n",
    "# # ParamGrid for Cross Validation\n",
    "# paramGrid = (ParamGridBuilder()\n",
    "#              .addGrid(selector.numTopFeatures, [2000, 500])  # number of features selected\n",
    "#              .addGrid(svm.regParam, [0.01, 0.1, 1.0])  # Regularization parameter\n",
    "#              .addGrid(svm.maxIter, [10, 50])  # Maximum number of iterations\n",
    "#              .build())\n",
    "\n",
    "# \n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions on test documents and print columns of interest.\n",
    "prediction = model.transform(test_df)\n",
    "selected = prediction.select(\"asin\", \"reviewText\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    rid, text, prob, prediction = row\n",
    "    print(\n",
    "        \"(%d, %s) --> prob=%s, prediction=%f\" % (\n",
    "            rid, text, str(prob), prediction   # type: ignore\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570fb79d-8b43-4c9a-832b-9726562eeee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fcbc5b-34be-4d9d-956c-f1199e58dd4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (DIC24)",
   "language": "python",
   "name": "python3_dic24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
