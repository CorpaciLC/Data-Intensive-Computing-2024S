{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f48c682-bf56-4d47-b6ca-0217c7c49e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import RegexTokenizer, CountVectorizer, StopWordsRemover, HashingTF, IDF, ChiSqSelector\n",
    "from pyspark.ml.feature import Tokenizer, Normalizer, StringIndexer\n",
    "from pyspark.sql.functions import col, lower\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "from pyspark.mllib.util import MLUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edabdac6-5661-4f82-adba-529e2dccc934",
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_dataset = \"/user/dic24_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "full_dataset = \"/user/dic24_shared/amazon-reviews/full/reviewscombined.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfe3b94a-5b55-4792-8a21-e4abf86f9687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "24/05/24 14:54:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/24 14:54:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/05/24 14:54:27 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/05/24 14:54:27 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "24/05/24 14:54:27 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "24/05/24 14:54:27 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "24/05/24 14:54:30 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File read successfully.\n",
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- helpful: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      "\n",
      "+----------+--------------------+-------+-------+--------------------+-----------+--------------+--------------------+-------------------+--------------+\n",
      "|      asin|            category|helpful|overall|          reviewText| reviewTime|    reviewerID|        reviewerName|            summary|unixReviewTime|\n",
      "+----------+--------------------+-------+-------+--------------------+-----------+--------------+--------------------+-------------------+--------------+\n",
      "|0981850006|Patio_Lawn_and_Garde| [6, 7]|    5.0|This was a gift f...| 12 3, 2009|A2VNYWOPJ13AFP|Amazon Customer \"...|             Delish|    1259798400|\n",
      "|B00002N66D|Patio_Lawn_and_Garde| [1, 1]|    5.0|This is a very ni...| 12 3, 2012|A2E5XXXC07AGA7|               James|      Nice spreader|    1354492800|\n",
      "|B00002N67U|Patio_Lawn_and_Garde| [0, 1]|    1.0|The metal base wi...|08 13, 2008|A16PX63WZIEQ13|             Finaldx|Terrible spike base|    1218585600|\n",
      "|B00002N6AN|Patio_Lawn_and_Garde| [0, 0]|    4.0|For the most part...| 10 1, 2009|A2OSWM3522VARA|Wayne Allen \"Moto...|  gets the job done|    1254355200|\n",
      "|B00002N8K3|Patio_Lawn_and_Garde| [4, 5]|    1.0|This hose is supp...|07 13, 2013|A2SX9YPPGEUADI|HappyCamper \"Happ...|          The worst|    1373673600|\n",
      "+----------+--------------------+-------+-------+--------------------+-----------+--------------+--------------------+-------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TFIDF_ChiSq_Selection\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "try:\n",
    "    df = spark.read.json(partial_dataset)\n",
    "    print(\"File read successfully.\")\n",
    "    \n",
    "    # Show the schema and some data\n",
    "    df.printSchema()\n",
    "    df.show(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c0a6459-6438-417e-bfd7-6e3f4a8e7e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------+-------+--------------------+-----------+--------------+--------------------+-------------------+--------------+\n",
      "|      asin|            category|helpful|overall|          reviewText| reviewTime|    reviewerID|        reviewerName|            summary|unixReviewTime|\n",
      "+----------+--------------------+-------+-------+--------------------+-----------+--------------+--------------------+-------------------+--------------+\n",
      "|0981850006|Patio_Lawn_and_Garde| [6, 7]|    5.0|this was a gift f...| 12 3, 2009|A2VNYWOPJ13AFP|Amazon Customer \"...|             Delish|    1259798400|\n",
      "|B00002N66D|Patio_Lawn_and_Garde| [1, 1]|    5.0|this is a very ni...| 12 3, 2012|A2E5XXXC07AGA7|               James|      Nice spreader|    1354492800|\n",
      "|B00002N67U|Patio_Lawn_and_Garde| [0, 1]|    1.0|the metal base wi...|08 13, 2008|A16PX63WZIEQ13|             Finaldx|Terrible spike base|    1218585600|\n",
      "|B00002N6AN|Patio_Lawn_and_Garde| [0, 0]|    4.0|for the most part...| 10 1, 2009|A2OSWM3522VARA|Wayne Allen \"Moto...|  gets the job done|    1254355200|\n",
      "|B00002N8K3|Patio_Lawn_and_Garde| [4, 5]|    1.0|this hose is supp...|07 13, 2013|A2SX9YPPGEUADI|HappyCamper \"Happ...|          The worst|    1373673600|\n",
      "+----------+--------------------+-------+-------+--------------------+-----------+--------------+--------------------+-------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# casefolding\n",
    "df = df.withColumn(\"reviewText\", lower(col(\"reviewText\")))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e35af735-fef1-43f3-a068-d41507318aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set count: 55401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set count: 11621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set count: 11807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_df, test_df, validation_df = df.randomSplit([0.7, 0.15, 0.15], seed=42)\n",
    "\n",
    "\n",
    "# Show the count of each set\n",
    "print(\"Training set count:\", train_df.count())\n",
    "print(\"Test set count:\", test_df.count())\n",
    "print(\"Validation set count:\", validation_df.count())\n",
    "# Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "# test = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3656aaa0-faa5-49a4-b4de-8c9653b41613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert category to numeric\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "\n",
    "# 1. tokenize\n",
    "tokenizer = RegexTokenizer(inputCol=\"reviewText\", outputCol=\"words\", pattern=r'\\s+|\\t+|\\d+|[(){}.!?,;:+=-_\"\\`~#@&*%€$§\\\\/]+', gaps=True)\n",
    "\n",
    "# 3. stopwords removal\n",
    "stopwords_remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"filtered_words\")\n",
    "\n",
    "## 4. tf-idf calculation with CountVectorizer\n",
    "# vectorizer = CountVectorizer(inputCol=stopwords_remover.getOutputCol(), outputCol=\"rawFeatures\", vocabSize=20000)\n",
    "# idf = IDF(inputCol=vectorizer.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "# 4. TF-IDF calculation with HashingTF\n",
    "hashingTF = HashingTF(inputCol=stopwords_remover.getOutputCol(), outputCol=\"rawFeatures\", numFeatures=20)\n",
    "\n",
    "\n",
    "# Chi-square\n",
    "selector = ChiSqSelector(numTopFeatures=2000, featuresCol=hashingTF.getOutputCol(),\n",
    "                         outputCol=\"selectedFeatures\", labelCol=indexer.getOutputCol())\n",
    "\n",
    "\n",
    "######\n",
    "# Part 3 starts here: \n",
    "######\n",
    "\n",
    "\n",
    "\n",
    "# Normalize in L^2\n",
    "# normalizer = Normalizer(p=2.0)\n",
    "# data1 = labels.zip(normalizer.transform(features))\n",
    "\n",
    "\n",
    "# SVM Classifier\n",
    "# svm = LinearSVC(featuresCol=\"selectedFeatures\", labelCol=indexer.getOutputCol(), maxIter=10)\n",
    "svm = LinearSVC(maxIter=10, regParam=0.1)\n",
    "ovr = OneVsRest(classifier=svm, featuresCol=\"selectedFeatures\", labelCol=indexer.getOutputCol())\n",
    "\n",
    "\n",
    "# Pipeline\n",
    "# pipeline = Pipeline(stages=[indexer, tokenizer, stopwords_remover, hashingTF, selector, normalizer, svm])\n",
    "pipeline = Pipeline(stages=[indexer, tokenizer, stopwords_remover, hashingTF, selector, ovr])\n",
    "\n",
    "# # ParamGrid for Cross Validation\n",
    "# paramGrid = (ParamGridBuilder()\n",
    "#              .addGrid(selector.numTopFeatures, [2000, 500])  # number of features selected\n",
    "#              .addGrid(svm.regParam, [0.01, 0.1, 1.0])  # Regularization parameter\n",
    "#              .addGrid(svm.maxIter, [10, 50])  # Maximum number of iterations\n",
    "#              .build())\n",
    "\n",
    "# \n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions on test documents and print columns of interest.\n",
    "prediction = model.transform(test_df)\n",
    "selected = prediction.select(\"asin\", \"reviewText\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    rid, text, prob, prediction = row\n",
    "    print(\n",
    "        \"(%d, %s) -->  prediction=%f\" % (\n",
    "            rid, text, str(prob), prediction   # type: ignore\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570fb79d-8b43-4c9a-832b-9726562eeee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fcbc5b-34be-4d9d-956c-f1199e58dd4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (DIC24)",
   "language": "python",
   "name": "python3_dic24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
